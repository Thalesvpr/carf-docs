# HEALTH CHECKS

Sistema CARF implementa endpoints de health checks readiness probes e liveness probes para cada serviço permitindo monitoramento automático do status de saúde dos componentes em orquestração Kubernetes Docker Compose com balanceamento de carga detecção de falhas e restart automático de pods unhealthy garantindo alta disponibilidade com SLA de noventa e nove ponto nove por cento em produção recovery automático de transient failures como database temporariamente indisponível cache offline ou serviços externos em timeout onde configuração adequada de timeouts evita false positives e cascade failures verificando dependencies críticos através de shallow health checks básicos validando processo running e deep health checks validando dependencies completas como PostgreSQL Redis Keycloak RabbitMQ. Backend GEOAPI expõe endpoint GET /health realizando verificação de saúde básica da aplicação retornando HTTP 200 OK quando aplicação está healthy e capaz de processar requests com payload JSON contendo campo status com enum valores Healthy Degraded Unhealthy, timestamp em formato ISO 8601 UTC, version do assembly informacional, e environment correspondente à configuração ASPNETCORE_ENVIRONMENT facilitando troubleshooting e identificação de instância ou pod específico ao correlacionar logs de múltiplas réplicas em cluster Kubernetes, além de endpoint GET /health/dependencies realizando verificação profunda de dependencies críticos retornando HTTP 200 OK se todos dependencies estão healthy ou 503 Service Unavailable se algum dependency falhou com payload JSON contendo status geral Degraded e array checks detalhando cada dependency individualmente com campos name status duration description e exception opcional permitindo troubleshooting preciso identificando exatamente qual componente falhou sem necessidade de inspecionar logs verbose ou metrics com correlação via Prometheus Grafana permitindo alerting baseado em unhealthy checks específicos como PostgreSQL down ou Redis timeout. Verificação de PostgreSQL no GEOAPI executa query pg_isready como alternativa a SELECT 1 simple query validando que connection pool está disponível através de Entity Framework Core DbContext método CanConnect com timeout configurado cinco segundos evitando hang indefinido caso database esteja indisponível ou network partition ocorra fazendo catch de SqlException logando error e retentando connection string validando que credenciais estão corretas e firewall rules permitem tráfego, verificação de Redis executa comando PING esperando resposta PONG através de StackExchange.Redis library verificando connection multiplexer IsConnected check com timeout de cinco segundos retry exponential backoff fazendo catch de RedisConnectionException logando warning entrando em degraded mode onde application continua funcionando sem cache com performance impacto aceitável usando feature flags para disable cache queries fazendo fallback para database direto, verificação de Keycloak chama endpoint nativo /health/ready usando URL base KEYCLOAK_URL com HttpClient request timeout cinco segundos expecting HTTP 200 validando que SSO está disponível usuários podem autenticar e tokens JWT são validados corretamente fazendo catch de HttpRequestException logando error e fazendo fallback para validação local de tokens usando cached public keys JWKS sem contatar Keycloak online operando em modo degraded onde autenticação continua funcionando mas refresh de tokens pode falhar. Endpoint GET /ready serve como readiness probe para Kubernetes determinando se pod está pronto para receber tráfego do load balancer incluindo todas verificações de dependencies retornando 503 se qualquer dependency crítico falhou impedindo roteamento de requests para pod que ainda está inicializando durante warm-up como EF Core migrations pending connection pools sendo estabelecidos ou caches sendo preloaded evitando errors 500 nos primeiros requests devido cold start, timeout global para health checks configurado em cinco segundos evita cascade failures onde health check próprio entra em timeout aguardando dependency lento infinitamente bloqueando threads do ASP.NET Core ThreadPool causando starvation executando checks em parallel usando async await Task.WhenAll para verificações simultâneas reduzindo latência total do health endpoint para cinco segundos versus quinze segundos se executado sequencialmente o que seria inaceitável, retry políticas para health checks de internal dependencies configuradas com uma tentativa sem retry permitindo decision rápida de unhealthy evitando delays e false positives onde transient network blip de milliseconds não deve marcar pod como unhealthy causando restart desnecessário usando stability threshold de três consecutive failures antes de marcar unhealthy evitando flapping. Database PostgreSQL implementa verificação de saúde executando comando pg_isready menos h localhost menos U postgres disponível no container image oficial PostgreSQL retornando exit code zero se database está aceitando connections code um se está rejeitando code dois se sem resposta timeout com alternativa psql menos U postgres menos c SELECT 1 query simples validando não apenas que connection listener está responding mas que database está operacional com queries executando corretamente filesystem íntegro sem corruption ou disk failures, Docker Compose healthcheck configurado com test CMD-SHELL pg_isready menos h localhost menos U variável POSTGRES_USER interval dez segundos timeout cinco segundos retries três start_period trinta segundos aguardando inicialização completa onde initialization scripts em /docker-entrypoint-initdb.d foram executados extensions PostGIS carregadas schema criado indexes built e ready para aceitar connections do GEOAPI com depends_on condition service_healthy garantindo ordem de startup, Kubernetes StatefulSet configura livenessProbe exec command /bin/sh menos c pg_isready menos U postgres initialDelaySeconds trinta periodSeconds dez timeoutSeconds cinco failureThreshold três fazendo restart do pod se consecutive failures indicam crash corruption irrecuperável com readinessProbe similar initialDelaySeconds dez periodSeconds cinco failureThreshold três removendo pod dos endpoints do service load balancer enquanto unhealthy permitindo recovery backup restore ou maintenance window planned downtime, monitoring adicional inclui queries de performance via pg_stat_activity contando connections ativas versus max_connections limit com alerting em oitenta por cento de capacidade indicando connection pool exhaustion, slow queries identificadas via pg_stat_statements queries maiores que um segundo para otimização de indexes vacuum analyze maintenance, replication lag via pg_stat_replication verificando standby replicas quantos segundos atrás do primary para failover automático usando Patroni consensus quorum. Keycloak expõe endpoint nativo GET /health/ready incluído desde Keycloak 18 mais com health checks habilitados via environment variable KC_HEALTH_ENABLED igual true retornando HTTP 200 OK com payload JSON status UP e checks array validando database connection PostgreSQL usado para persistence de realms users sessions está disponível e Infinispan cache distribuído em clustered mode com nodes sincronizados e sessions replicadas, endpoint GET /health/live serve como liveness probe validando que processo Keycloak JVM está healthy sem deadlock OutOfMemoryError ou crash necessitando restart, Docker Compose healthcheck test CMD-SHELL curl menos menos fail http://localhost:8080/health/ready ou exit 1 interval trinta segundos timeout dez segundos retries três start_period sessenta segundos considerando Keycloak startup lento sendo Java application com JVM warm-up realm import e initial admin user creation devendo aguardar ready antes de marcar healthy, Kubernetes Deployment configura livenessProbe httpGet path /health/live port 8080 initialDelaySeconds cento e vinte periodSeconds trinta timeoutSeconds dez failureThreshold três com readinessProbe path /health/ready initialDelaySeconds sessenta periodSeconds dez timeoutSeconds cinco failureThreshold três removendo pod do load balancer durante startup enquanto import de realm carf-realm.json processa usuários roles clients e configurações são validadas, timeout generoso para Keycloak considerando cold start lento onde primeiros requests após deploy levam dois a três minutos para completar import de realm grande com milhares de usuários evitando false positive restart loop CrashLoopBackOff never ready devendo aumentar initialDelaySeconds para cento e oitenta em ambientes grandes de produção com milhares de usuários simultâneos sessions ativas e memory heap Xmx2g Xms1g com tuning JVM garbage collection G1GC e monitoring. Redis implementa verificação de saúde executando comando PING esperando resposta mais PONG barra r barra n indicando servidor operacional aceitando connections processando comandos corretamente com memory disponível eviction policy funcionando e persistence AOF RDB configurada, Docker Compose healthcheck test CMD redis-cli menos menos raw incr ping com alternativa CMD redis-cli ping interval cinco segundos timeout três segundos retries três start_period dez segundos considerando Redis startup rápido de segundos sendo lightweight key-value store em memory sem disk I/O ou initialization complexa, Kubernetes Deployment livenessProbe exec command redis-cli ping initialDelaySeconds dez periodSeconds cinco timeoutSeconds três failureThreshold três com readinessProbe similar parâmetros idênticos sendo Redis stateless cache onde perda de pod não é crítico pois session storage pode recuperar do backend database como fallback em degraded mode, autenticação configurada com requirepass secret REDIS_PASSWORD variável ambiente requer health check ajustar comando redis-cli menos a variável REDIS_PASSWORD ping com parsing de ACL users Redis 6 mais permitindo granular permissions com default user disabled e custom user health_checker com role ping permission apenas evitando exposure de password em command history ou logs, monitoring adicional inclui memory usage via INFO memory used_memory_human versus maxmemory com eviction alerting em noventa por cento de capacidade, eviction rate aumentando indicando cache miss ratio e degradação de performance com hit rate menor que oitenta por cento necessitando investigate de queries TTL e key patterns para optimization, cluster mode Redis Sentinel ou Redis Cluster health checks validam master reachable replicas synced e quorum available para failover automático em split-brain scenarios network partition usando consensus algorithm Raft. RabbitMQ implementa verificação de saúde usando comando rabbitmq-diagnostics ping retornando exit code zero se node está healthy com Erlang VM running broker accepting connections e queues operacionais com alternativa rabbitmq-diagnostics status fornecendo verbose output incluindo node name version uptime memory alarms disk alarms e clustering status, Docker Compose healthcheck test CMD rabbitmq-diagnostics ping interval trinta segundos timeout dez segundos retries três start_period sessenta segundos considerando RabbitMQ startup lento devido Erlang VM initialization plugins sendo habilitados como rabbitmq_management rabbitmq_shovel e clustering nodes joining devendo aguardar ready evitando false positive, Kubernetes Deployment livenessProbe exec command rabbitmq-diagnostics ping initialDelaySeconds sessenta periodSeconds trinta timeoutSeconds dez failureThreshold três com readinessProbe similar validando node no cluster está healthy accepting publishing consuming messages com queues sincronizadas usando mirrored queues ou quorum queues replicated, Management API fornece alternativa GET http://localhost:15672/api/healthchecks/node retornando JSON status ok com autenticação basic auth guest:guest em desenvolvimento mas credentials seguras em produção com permissions de monitoring para usuário apenas, alarms monitoring incluem memory alarm e disk alarm onde default RabbitMQ para de accepting publishers quando memory maior que threshold default quarenta por cento RAM ou disk menor que cinquenta GB como proteção prevent OutOfMemory crash e data loss mas consumers continuam draining queues enquanto publishers são bloqueados via backpressure até alarm cleared com alerting imediato via Prometheus rabbitmq_alarms_total metric com Grafana dashboard e PagerDuty notification para on-call engineer requerer intervenção manual para aumentar memory disk liberar espaço ou investigate memory leak de plugins. Frontend GEOWEB SPA servido por nginx implementa verificação de saúde retornando HTTP 200 ao acessar GET / root URL servindo index.html implicando que nginx está operacional arquivo index.html presente em /usr/share/nginx/html routing try_files funcionando corretamente e proxy_pass /api backend configurado sendo validação simples para stateless frontend sem dependencies de startup para validar além de nginx processo running e servindo arquivos estáticos, Docker Compose healthcheck test CMD curl menos menos fail http://localhost/ interval trinta segundos timeout cinco segundos retries três start_period dez segundos considerando nginx startup rápido de segundos com configuração lida e assets copiados em build time sendo container imutável, Kubernetes Deployment livenessProbe httpGet path / port 80 initialDelaySeconds dez periodSeconds dez timeoutSeconds cinco failureThreshold três com readinessProbe similar parâmetros idênticos permitindo stateless scaling horizontal com múltiplas replicas usando load balancer round-robin sem session affinity desnecessária pois JWT tokens stateless são validados no backend GEOAPI, monitoring adicional inclui nginx access logs e error logs com parsing de 4xx client errors e 5xx server errors indicando upstream backend failures proxy_pass timeout ou rate limiting 429 Too Many Requests com metrics via Prometheus nginx-exporter exportando requests_total http_request_duration_seconds upstream_response_time para latency P95 P99 cumprindo SLA de duzentos milissegundos, frontend assets servidos via CDN CloudFront edge caching reduzindo origin load com TTFB Time To First Byte e Core Web Vitals LCP FID CLS monitorados via Google Lighthouse CI com performance budget e alerts para degradation. Plugin GEOGIS para QGIS desktop não possui health endpoint HTTP tradicional por ser desktop application local do usuário sem servidor mas validação alternativa confirma plugin load successful sem initialization erro verificando QGIS startup plugin manager enabled não corrupted consultando logs QGIS message log em ~/.local/share/QGIS/QGIS3/profiles/default/log/qgis.log procurando warnings exceptions Python traceback PyQGIS API calls failed ou missing dependencies como requests urllib3 certifi SSL libraries, backend API connectivity test manual permite usuário acionar via toolbar botão Verify Connection disparando request GET /health backend com timeout dez segundos exibindo dialog Success indicando backend reachable version compatible ou Failure indicando network error firewall VPN required authentication expired necessitando re-login SSO Keycloak desktop OAuth2 flow refresh tokens, automated testing em CI/CD inclui unit tests via pytest para plugin Python code mocking PyQGIS QgsVectorLayer QgsProject dependencies, integration tests usando Docker QGIS headless com Xvfb virtual display rendering layers executando scripts processing algorithms validating outputs GeoPackage WKT geometries com assertions de correctness, E2E tests manual por QA testers seguindo checklist de funcionalidades críticas como loading layers filtering styling exporting reports validando Windows Linux compatibility matrix para QGIS versions 3.28 LTS e 3.34 latest verificando plugins API sem breaking changes ou deprecations. Configuração de timeout global para health check endpoints em cinco segundos evita cascade failures onde situação de dependency lento como PostgreSQL query hanging trinta segundos faria health check aguardar indefinitely bloqueando threads do ASP.NET Core ThreadPool causando esgotamento onde requests subsequentes entram em timeout retornando 503 Service Unavailable causando avalanche failure com sistema inteiro degradado por single slow component devendo usar timeouts curtos de cinco segundos para fail fast com decisão rápida de unhealthy permitindo restart e recovery automático via Kubernetes liveness probe kill pod start fresh instance evitando esperar minutes com hung process para liberar recursos, retry policies para health checks evitam agressividade usando uma tentativa única sem retry para decision definitiva de unhealthy onde transient failures de milliseconds como network blip ou packet loss não devem causar restart de pod usando stability threshold via Kubernetes failureThreshold três consecutive failures antes de action de restart ou remove load balancer aguardando trinta segundos com três checks em interval de dez segundos giving component chance recover de temporary issue implementando self-healing de distributed systems com resilience patterns como circuit breakers bulkheads e timeouts, graceful shutdown handling SIGTERM em Kubernetes pod deletion drain connections finish in-flight requests com timeout de trinta segundos graceful termination period via terminationGracePeriodSeconds default trinta após o qual timeout SIGKILL force kill processo abruptly com connections dropped e requests failed 5xx afetando users necessitando retry com configuração adequada na aplicação ASP.NET Core UseShutdownTimeout WebHost lifetime stopping event handlers para cleanup de resources close database connections flush logs Serilog e dispose IDisposable services do DI container, health check startup probes Kubernetes startupProbe adicional para slow-starting applications como Keycloak com Java heap initialization realm import de large datasets usando failureThreshold trinta periodSeconds dez totalizando trezentos segundos cinco minutos permitindo startup completo antes de liveness e readiness probes assumir controle evitando premature restart CrashLoopBackOff loop never ready com increasing backoff exponential delays de seconds para minutes onde pod never operational devendo startupProbe ter generous initial delay e failureThreshold protecting cold start scenarios. Integração de health checks com Prometheus metrics exposition endpoint /metrics do GEOAPI exportando métricas health_check_duration_seconds histogram mostrando latency P50 P95 P99 e health_check_status gauge com valor um para healthy zero para unhealthy separado por dependency como PostgreSQL Redis Keycloak individualmente permitindo alerting via Prometheus AlertManager rules firing quando unhealthy maior que cinco minutos com severity critical disparando notification para Slack Microsoft Teams ou PagerDuty on-call engineer com escalation e runbook steps para troubleshooting, Grafana dashboards fornecem visualização real-time de health status de todos componentes do sistema usando grid de panels com green para healthy red para unhealthy yellow para degraded mostrando histórico de uptime percentage para cálculo de SLA noventa e nove ponto nove por cento com downtime incidents duration permitindo RCA root cause analysis postmortem documentation e prevention measures, Kubernetes events via kubectl get events watching pod restarts CrashLoopBackOff com reasons como Liveness probe failed Readiness probe failed OOMKilled exceeded memory limits ou eviction por node pressure DiskPressure MemoryPressure permitindo correlação de health check failures com logs stderr stdout via kubectl logs pod-name previous para crashed container debugging, audit logs de health check failures são registrados em database tabela health_check_audit com campos timestamp component status duration error_message stack_trace para compliance e investigation de patterns como recurring failures em specific times do dia como load peak hours permitindo capacity planning e scaling horizontal vertical via HPA metrics-based autoscaling baseado em CPU memory custom metrics como request rate ou queue length.

---

**Última atualização:** 2026-01-10
