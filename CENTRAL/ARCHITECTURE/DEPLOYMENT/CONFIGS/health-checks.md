# HEALTH CHECKS

Documentação endpoints health checks readiness probes liveness probes por serviço sistema CARF permitindo monitoramento automático status saúde componentes orquestração Kubernetes Docker Compose balanceamento carga detecção falhas restart automático pods unhealthy garantindo alta disponibilidade SLA 99.9% produção recovery automático transient failures database indisponível cache offline serviços externos timeout configuração adequada evitar false positives cascade failures check dependencies críticos shallow deep health validação.

## GEOAPI

Endpoint GET /health expondo verificação saúde básica aplicação retornando HTTP 200 OK quando aplicação healthy capaz processar requests payload JSON formato {"status": "Healthy", "timestamp": "2025-01-07T10:30:00Z", "version": "1.2.3", "environment": "Production"} campo status enum valores Healthy Degraded Unhealthy timestamp ISO 8601 UTC version assembly informacional environment configuração ASPNETCORE_ENVIRONMENT facilitando troubleshooting identificação instância pod específico logs correlacionados. Endpoint GET /health/dependencies realizando verificação profunda dependencies críticos retornando HTTP 200 OK se todos healthy 503 Service Unavailable se algum dependency falhou payload JSON formato {"status": "Degraded", "checks": [{"name": "PostgreSQL", "status": "Healthy", "duration": "45ms", "description": "Connection successful"}, {"name": "Redis", "status": "Unhealthy", "duration": "5002ms", "description": "Connection timeout", "exception": "RedisConnectionException"}, {"name": "Keycloak", "status": "Healthy", "duration": "120ms", "description": "Health endpoint returned 200"}], "totalDuration": "5167ms"} array checks detalhando cada dependency individualmente status duration description exception opcional troubleshooting identificar exatamente qual componente falhou sem necessidade inspecionar logs verbose metrics correlação Prometheus Grafana alerting baseado unhealthy checks específicos. Verificação PostgreSQL executando query pg_isready alternativa SELECT 1 simple query validando connection pool disponível Entity Framework Core DbContext CanConnect timeout configurado 5 segundos evitar hang indefinido database indisponível network partition catch SqlException log error retry connection string validation credenciais corretas firewall rules permitem tráfego. Verificação Redis executando comando PING esperando resposta PONG StackExchange.Redis library connection multiplexer IsConnected check timeout 5s retry exponential backoff catch RedisConnectionException log warning degraded mode application continua funcionar sem cache performance impacto aceitável feature flags disable cache queries fallback database direto. Verificação Keycloak chamando endpoint nativo /health/ready URL base KEYCLOAK_URL HttpClient request timeout 5s expecting HTTP 200 validando SSO disponível usuários podem autenticar tokens JWT validados corretamente catch HttpRequestException log error fallback validação local tokens cached public keys JWKS sem contatar Keycloak online modo degraded autenticação continua refresh tokens pode falhar. Endpoint GET /ready readiness probe Kubernetes determinando se pod pronto receber tráfego load balancer incluindo verificações dependencies retornando 503 se qualquer dependency crítico falhou impedindo roteamento requests pod ainda inicializando warm-up EF Core migrations pending connection pools estabelecendo caches preloading evitar errors 500 primeiros requests cold start. Timeout global health checks configurado 5 segundos evitar cascade failures health check próprio timeout aguardando dependency lento infinitamente bloqueando threads ASP.NET Core ThreadPool starvation parallel checks async await Task.WhenAll verificações simultâneas reduzir latência total health endpoint 5s individual 15s sequencial inaceitável. Retry políticas health checks internal dependencies 1 tentativa sem retry decision rápida unhealthy evitar delays false positives transient network blip milliseconds não deve marcar pod unhealthy restart desnecessário stability threshold 3 consecutive failures antes marcar unhealthy evitar flapping.

## PostgreSQL

Verificação saúde executando comando pg_isready -h localhost -U postgres disponível container image oficial PostgreSQL retornando exit code 0 se database aceitando connections code 1 se rejeitando code 2 se sem resposta timeout alternativa psql -U postgres -c "SELECT 1" query simples validando não apenas connection listener responding mas database operacional queries executando corretamente filesystem íntegro corruption disk failures. Docker Compose healthcheck configurado test ["CMD-SHELL", "pg_isready -h localhost -U $${POSTGRES_USER}"] interval 10s timeout 5s retries 3 start_period 30s aguardando inicialização completa initialization scripts /docker-entrypoint-initdb.d executados extensions PostGIS carregadas schema criado indexes built ready aceitar connections GEOAPI depends_on condition service_healthy garantindo ordem startup. Kubernetes StatefulSet livenessProbe exec command ["/bin/sh", "-c", "pg_isready -U postgres"] initialDelaySeconds 30 periodSeconds 10 timeoutSeconds 5 failureThreshold 3 restart pod se consecutive failures indicando crash corruption irrecuperável readinessProbe similar initialDelaySeconds 10 periodSeconds 5 failureThreshold 3 remover pod endpoints service load balancer enquanto unhealthy recovery backup restore maintenance window planned downtime. Monitoring adicional queries performance pg_stat_activity contando connections ativas max_connections limit alerting 80% capacidade connection pool exhaustion slow queries pg_stat_statements identificando queries > 1s otimização indexes vacuum analyze maintenance replication lag pg_stat_replication standby replicas segundos atrás primary failover automático Patroni consensus quorum.

## Keycloak

Endpoint nativo GET /health/ready incluído Keycloak 18+ health checks habilitados environment variable KC_HEALTH_ENABLED=true retornando HTTP 200 OK payload JSON {"status": "UP", "checks": [{"name": "Database", "status": "UP"}, {"name": "Infinispan", "status": "UP"}]} validando database connection PostgreSQL usado persistence realms users sessions disponível Infinispan cache distribuído clustered mode nodes sincronizados sessions replicadas. Endpoint GET /health/live liveness probe validando processo Keycloak JVM healthy não deadlock OutOfMemoryError crash restart necessário. Docker Compose healthcheck test ["CMD-SHELL", "curl --fail http://localhost:8080/health/ready || exit 1"] interval 30s timeout 10s retries 3 start_period 60s Keycloak startup lento Java application JVM warm-up realm import initial admin user creation aguardar ready antes marcar healthy. Kubernetes Deployment livenessProbe httpGet path /health/live port 8080 initialDelaySeconds 120 periodSeconds 30 timeoutSeconds 10 failureThreshold 3 readinessProbe path /health/ready initialDelaySeconds 60 periodSeconds 10 timeoutSeconds 5 failureThreshold 3 remover pod load balancer enquanto startup import realm carf-realm.json processando usuários roles clients configurações validadas. Timeout generoso Keycloak lento cold start primeiros requests após deploy 2-3 minutos completo import realm grande milhares usuários evitar false positive restart loop CrashLoopBackOff never ready aumentar initialDelaySeconds 180 ambientes grandes produção milhares usuários simultâneos sessions ativas memory heap Xmx2g Xms1g tuning JVM garbage collection G1GC monitoring.

## Redis

Verificação saúde executando comando Redis PING esperando resposta +PONG\r\n indicando servidor operacional aceitando connections processando comandos corretamente memory disponível eviction policy funcionando persistence AOF RDB configurada. Docker Compose healthcheck test ["CMD", "redis-cli", "--raw", "incr", "ping"] alternativa ["CMD", "redis-cli", "ping"] interval 5s timeout 3s retries 3 start_period 10s Redis startup rápido segundos lightweight key-value store memory sem disk I/O initialization complexa. Kubernetes Deployment livenessProbe exec command ["redis-cli", "ping"] initialDelaySeconds 10 periodSeconds 5 timeoutSeconds 3 failureThreshold 3 readinessProbe similar parâmetros idênticos Redis stateless cache perda pod não crítico session storage pode recuperar backend database fallback degraded mode. Autenticação configurada requirepass secret REDIS_PASSWORD variável ambiente health check ajustar comando redis-cli -a $REDIS_PASSWORD ping parsing ACL users Redis 6+ granular permissions default user disabled custom user health_checker role ping permission apenas evitar exposure password command history logs. Monitoring adicional memory usage INFO memory used_memory_human maxmemory eviction alerting 90% capacidade eviction rate aumentando cache miss ratio degradação performance hit rate < 80% investigate queries TTL key patterns optimization. Cluster mode Redis Sentinel Cluster health checks validar master reachable replicas synced quorum available failover automático split-brain scenarios network partition consensus algorithm Raft.

## RabbitMQ

Verificação saúde comando rabbitmq-diagnostics ping retornando exit code 0 se node healthy Erlang VM running broker accepting connections queues operacionais alternativa rabbitmq-diagnostics status verbose output node name version uptime memory alarms disk alarms clustering status. Docker Compose healthcheck test ["CMD", "rabbitmq-diagnostics", "ping"] interval 30s timeout 10s retries 3 start_period 60s RabbitMQ startup lento Erlang VM initialization plugins habilitando rabbitmq_management rabbitmq_shovel clustering nodes joining aguardar ready evitar false positive. Kubernetes Deployment livenessProbe exec command ["rabbitmq-diagnostics", "ping"] initialDelaySeconds 60 periodSeconds 30 timeoutSeconds 10 failureThreshold 3 readinessProbe similar validar node cluster healthy accepting publishing consuming messages queues sincronizadas mirrored queues quorum queues replicated. Management API alternativa GET http://localhost:15672/api/healthchecks/node retornando JSON {"status": "ok"} autenticação basic auth guest:guest desenvolvimento credentials seguras produção permissions monitoring usuário apenas. Alarms monitoring memory alarm disco alarm default RabbitMQ para accepting publishers quando memory > threshold default 40% RAM disk < 50GB proteção prevent OutOfMemory crash data loss consumers continuam draining queues publishers bloqueados backpressure até alarm cleared alerting immediato Prometheus rabbitmq_alarms_total metric Grafana dashboard PagerDuty notification on-call engineer intervenção manual aumentar memory disk liberar espaço investigate memory leak plugins.

## GEOWEB

Verificação saúde frontend SPA servido nginx retornando HTTP 200 ao acessar GET / root URL servindo index.html implicando nginx operacional arquivo index.html presente /usr/share/nginx/html routing try_files funcionando corretamente proxy_pass /api backend configurado validação simples stateless frontend não dependencies startup validar além nginx processo running serve arquivos estáticos. Docker Compose healthcheck test ["CMD", "curl", "--fail", "http://localhost/"] interval 30s timeout 5s retries 3 start_period 10s nginx startup rápido segundos configuração lida assets copiados build time imutável container. Kubernetes Deployment livenessProbe httpGet path / port 80 initialDelaySeconds 10 periodSeconds 10 timeoutSeconds 5 failureThreshold 3 readinessProbe similar parâmetros idênticos stateless scaling horizontal múltiplas replicas load balancer round-robin session affinity desnecessária JWT tokens stateless backend GEOAPI. Monitoring adicional nginx access logs error logs parsing 4xx client errors 5xx server errors upstream backend failures proxy_pass timeout rate limiting 429 Too Many Requests metrics Prometheus nginx-exporter requests_total http_request_duration_seconds upstream_response_time latency P95 P99 SLA 200ms frontend assets CDN CloudFront edge caching reducing origin load TTFB Time To First Byte Core Web Vitals LCP FID CLS Google Lighthouse CI performance budget alerts degradation.

## GEOGIS

Verificação saúde plugin QGIS desktop não aplicável tradicional health endpoint HTTP servidor ausente desktop application local usuário validação alternativa plugin load successful initialization erro QGIS startup plugin manager disabled corrupted verificar logs QGIS message log ~/.local/share/QGIS/QGIS3/profiles/default/log/qgis.log warnings exceptions Python traceback PyQGIS API calls failed missing dependencies requests urllib3 certifi SSL libraries backend API connectivity test manual usuário toolbar botão Verify Connection disparando request GET /health backend timeout 10s exibindo dialog Success backend reachable version compatible Failure network error firewall VPN required authentication expired re-login SSO Keycloak desktop OAuth2 flow refresh tokens. Automated testing CI/CD unit tests pytest plugin Python code mocking PyQGIS QgsVectorLayer QgsProject dependencies integration tests Docker QGIS headless Xvfb virtual display rendering layers scripts processing algorithms validating outputs GeoPackage WKT geometries assertions correctness E2E tests manual QA testers checklist funcionalidades críticas loading layers filtering styling exporting reports Windows Linux compatibility matrix QGIS versions 3.28 LTS 3.34 latest plugins API breaking changes deprecations.

## Timeout Configuração

Timeout global health check endpoints configurado 5 segundos evitar cascade failures situação dependency lento PostgreSQL query hanging 30s health check aguardando indefinitely thread pool ASP.NET Core esgotado requests subsequentes timeout 503 Service Unavailable avalanche failure sistema inteiro degradado single slow component timeouts curtos 5s fail fast decisão rápida unhealthy restart recovery automático Kubernetes liveness probe kill pod start fresh instance esperando minutes hung process liberar recursos. Retry policies health checks evitar agressividade 1 tentativa única sem retry decision definitiva unhealthy transient failures milliseconds network blip packet loss não devem causar restart pod stability threshold Kubernetes failureThreshold 3 consecutive failures antes action restart remove load balancer aguardar 30s 3 checks 10s interval giving component chance recover temporary issue self-healing distributed systems resilience patterns circuit breakers bulkheads timeouts. Graceful shutdown handling SIGTERM Kubernetes pod deletion drain connections finish in-flight requests timeout 30s graceful termination period terminationGracePeriodSeconds default 30 após timeout SIGKILL force kill processo abruptly connections dropped requests failed 5xx users retry configuração adequada aplicação ASP.NET Core UseShutdownTimeout WebHost lifetime stopping event handlers cleanup resources close database connections flush logs Serilog dispose IDisposable services DI container. Health check startup probes Kubernetes startupProbe adicional slow-starting applications Keycloak Java heap initialization realm import large datasets failureThreshold 30 periodSeconds 10 total 300s 5 minutes permitindo startup completo antes liveness readiness probes assumir controle evitar premature restart CrashLoopBackOff loop never ready increasing backoff exponential delays seconds minutes pod never operational startupProbe generous initial delay failureThreshold protecting cold start scenarios.

## Monitoring Integração

Health checks integrados Prometheus metrics exposition endpoint /metrics GEOAPI exportando métricas health_check_duration_seconds histogram latency P50 P95 P99 health_check_status gauge 1 healthy 0 unhealthy por dependency PostgreSQL Redis Keycloak separadamente alerting Prometheus AlertManager rules firing unhealthy > 5 minutes severity critical notification Slack Microsoft Teams PagerDuty on-call engineer escalation runbook steps troubleshooting. Grafana dashboards visualização real-time health status todos componentes sistema grid panels green healthy red unhealthy yellow degraded histórico uptime percentage SLA 99.9% calculation downtime incidents duration RCA root cause analysis postmortem documentation prevention measures. Kubernetes events kubectl get events watching pod restarts CrashLoopBackOff reasons Liveness probe failed Readiness probe failed OOMKilled exceeded memory limits eviction node pressure DiskPressure MemoryPressure correlação health check failures logs stderr stdout kubectl logs pod-name previous crashed container debugging. Audit logs health check failures registrados database tabela health_check_audit timestamp component status duration error_message stack_trace compliance investigation patterns recurring failures specific times day load peak hours capacity planning scaling horizontal vertical HPA metrics-based autoscaling CPU memory custom metrics request rate queue length.

---

**Última atualização:** 2025-01-07
